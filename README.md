# Baidu Baike Scraper

A Python tool to scrape content from Baidu Baike (百度百科) pages and save them in markdown and JSON formats.

## Features

- Scrapes Baidu Baike pages for:
  - Title
  - Short description
  - Abstract
  - Information box
  - Table of contents
  - Main content
  - References
- Handles citations in content:
  - Creates a clean version without citation tags
  - Creates a version with properly formatted citations in [1, 2, 3] format
- Saves data in three formats:
  - Raw HTML
  - Markdown (both with and without citations)
  - JSON (includes structured data and markdown content)
- Creates a separate folder for each scrape to keep data organized
- Implements rate limiting and retry logic to be respectful to the server
- Uses random user agents to mimic human behavior

## Requirements

```
pip install -r requirements.txt
```

## Usage

### Basic Usage (NOT RECOMMENDED, see [Selenium Scraper](#selenium-scraper) below)

```bash
python baidu_scraper.py "https://baike.baidu.com/item/词条1" "https://baike.baidu.com/item/词条2"
```

### Using a URL File

Create a text file with one URL per line:

```
https://baike.baidu.com/item/词条1
https://baike.baidu.com/item/词条2
https://baike.baidu.com/item/词条3
```

Then run:

```bash
python baidu_scraper.py -f urls.txt
```

### Default URL

If no URLs are provided, the script will use the default URL:
- `https://baike.baidu.com/item/华为技术有限公司/6455903`

## Output

For each URL, the script creates a folder in the `output` directory with:

1. `raw.html` - The raw HTML of the page
2. `content.md` - The formatted markdown content without citations
3. `content_with_citations.md` - Formatted markdown content with citations in [1, 2, 3] format
4. `data.json` - Structured JSON data including:
   - All extracted data in structured format
   - Complete markdown content as a single string (both clean and with citations)
   - Citations properly formatted in [1, 2, 3] style

## Notes

- The script includes random delays between requests to avoid overloading the server
- Error handling and logging are implemented to help debug issues
- If a scrape fails, the script will continue with the next URL 

## Selenium Scraper

A new Selenium-based scraper is now available that can handle dynamically loaded content, including hidden references that may not be accessible with the basic scraper.

### Features

- Uses Selenium WebDriver to render JavaScript and access dynamically loaded content
- Automatically waits for important page elements to load
- Additional retry mechanism for reference extraction
- Same structured output as the basic scraper (JSON, Markdown)
- Excel export capability for comparing multiple articles

### Requirements

Make sure you have Chrome installed on your system. The scraper uses Chrome WebDriver which will be automatically downloaded by the `webdriver-manager` package.

### Usage

```bash
# Scrape a single page
python selenium_baidu_scraper.py "https://baike.baidu.com/item/example"

# Scrape multiple pages
python selenium_baidu_scraper.py "https://baike.baidu.com/item/example1" "https://baike.baidu.com/item/example2"

# Save to a specific output directory
python selenium_baidu_scraper.py --output "my_output_dir" "https://baike.baidu.com/item/example"

# Export data to Excel
python selenium_baidu_scraper.py --excel "data.xlsx" "https://baike.baidu.com/item/example"
```

### Differences from Basic Scraper

The Selenium-based scraper can access content that requires JavaScript execution, such as:

1. Dynamically loaded reference sections
2. Hidden content that becomes visible after JavaScript execution
3. Content generated by client-side rendering

This makes it more reliable for pages with complex interactive elements or lazy-loaded content. 